# 产品需求文档 (PRD): Mineru2Questions

**版本**: 2.0
**状态**: 定稿
**修订日期**: 2026-02-15
**作者**: Manus AI

> **自包含声明**: 本文档是 Mineru2Questions 项目的唯一标准文档。它涵盖了项目的全部背景、目标、功能需求、技术规格和数据定义，不依赖于任何外部文档、历史对话或代码注释。任何与本文档冲突的外部信息，以本文档为准。

---

## 修订历史

| 版本 | 日期 | 修订人 | 主要变更 |
| :--- | :--- | :--- | :--- |
| 1.0 | 2026-01-15 | 初始团队 | 初版，复刻 DataFlow 官方流水线，尝试同时抽取题目和远距离答案 |
| 1.1 | 2026-01-28 | 初始团队 | 简化为仅抽取题目，放弃远距离答案匹配 |
| 2.0 | 2026-02-15 | Manus AI | **重大重构**。引入"双 LLM 流水线"，增加章节预处理与验证容错算子；将 ChapterOverwrite 修订为 ChapterMerge（融合而非强制覆盖）；新增 MinerU 解析前置步骤；补充完整前端 UI/UX 需求；按照"十步法"标准重写为全面、完整、自包含的唯一标准文档；整合历史 15 份技术文档的全部关键发现与教训 |

---

## 目录

1.  [产品概述与战略演进](#1-产品概述与战略演进)
2.  [目标用户与使用场景](#2-目标用户与使用场景)
3.  [产品原则](#3-产品原则)
4.  [功能需求：前端与用户交互 (UI/UX)](#4-功能需求前端与用户交互-uiux)
5.  [功能需求：后端核心流水线](#5-功能需求后端核心流水线)
6.  [系统架构与技术栈](#6-系统架构与技术栈)
7.  [发布标准](#7-发布标准)
8.  [开发路线图](#8-开发路线图)
9.  [附录 A：数据模型 (TypeScript 类型定义)](#9-附录-a数据模型-typescript-类型定义)
10. [附录 B：数据库 Schema (Drizzle ORM)](#10-附录-b数据库-schema-drizzle-orm)
11. [附录 C：API 接口 (tRPC 路由)](#11-附录-capi-接口-trpc-路由)
12. [附录 D：LLM 预设提供商](#12-附录-dllm-预设提供商)
13. [附录 E：文件系统约定](#13-附录-e文件系统约定)
14. [附录 F：已知问题与限制](#14-附录-f已知问题与限制)
15. [附录 G：历史关键发现与教训总结](#15-附录-g历史关键发现与教训总结)

---

## 1. 产品概述与战略演进

### 1.1 价值主张 (Elevator Pitch)

**对于**需要将海量、非结构化的 PDF 教育文档（如教材、讲义、试卷、练习册）转化为高质量、结构化数据的**数据工程师和 AI 产品团队**，**Mineru2Questions 是一个**高保真结构化内容解析引擎，**它能够**通过一个鲁棒的、可观测的"双 LLM 流水线"，精确地提取题目内容并还原其在原始文档中的章节归属关系。**不同于**那些仅做简单 OCR 或依赖脆弱规则的传统工具，**我们的产品**提供了工业级的可靠性和可追溯性，确保每一条提取数据的准确性和结构完整性，为下游智能题库、学习分析和 AI 助教等应用提供最坚实的数据基石。

### 1.2 项目演进历史

本项目基于 OpenDCAI/DataFlow 官方仓库（`PDF_VQA_extract_optimized_pipeline`）进行二次开发，经历了四个关键阶段。以下是完整的演进历程，包含每个阶段的核心发现和教训。

**第一阶段：复刻官方仓库 (v1.0 - v1.1)**

团队完整复刻了 DataFlow 官方流水线的核心算子（`MinerU2LLMInputOperator`、`LLMOutputParser`、`QA_Merger`），并增加了 Web 交互界面和任务管理功能。v1.0 尝试同时抽取题目和远距离答案（即题目在正文、答案在书末的场景），发现远距离答案的匹配准确率极低。v1.1 做出关键产品决策：**放弃远距离答案匹配，聚焦于高质量题目抽取**，仅保留与题目紧邻的例题解答（50 Block 以内）。

**第二阶段：修复阻塞性缺陷 (v1.2 - v1.6)**

这一阶段集中解决了多个导致系统无法正常工作的严重缺陷：

- **v1.2 — 输入格式化严重偏差**: 发现 MinerU 输出的 `list`、`equation` 等复杂类型未被正确展平，噪声块（页码、页眉页脚）未被过滤，导致 ID 不连续、LLM 输入质量低。核心结论是"**优先修复数据输入端，而不是调整提示词**"。引入了 `BlockFlattener` 算子进行统一展平。
- **v1.3 — API 端点回归**: 重构过程中 LLM 调用的 `/chat/completions` 路径丢失，加上任务目录路径错误，导致 0 题产出。
- **v1.4 — 超时单位错误**: `timeout` 参数的单位从秒被错误地改为毫秒（300ms），导致所有 LLM 调用超时失败，0 题产出。
- **v1.5 — 性能回归**: 并发池被错误地退化为串行执行（`for...await` 替代了并发机制），处理时间从 2.4 分钟膨胀到 15 分钟。修复后恢复并发池 + `maxWorkers` 控制。
- **v1.6 — 里程碑**: 首次成功产出 851 道题目。同时暴露了新问题：章节标题噪声（365/851 = 42.9% 为噪声）、去重不足、子题拆分不当、`extractedCount` 未更新。

**第三阶段：数据质量优化 (v1.8 - v2.0)**

这一阶段聚焦于提升抽取结果的数据质量：

- **v1.8 — 去重过度**: 使用 `(title, label)` 二元组去重，因 `title` 不稳定导致过度去重，题目数从 851 降至 509（丢失 40%）。
- **v1.9 — 去重策略修正**: 改为基于 `questionIds`（Block ID 序列）去重，题目数恢复至 998，54 组真正重复被正确识别。但章节标题质量仍有 32.6% 噪声。
- **v2.0 — 去重稳定 + Sanity Check**: 去重完全修复，846 题稳定产出。发现 LLM 注意力衰减问题（某些大 Chunk 只输出 1 道题），实现了 Sanity Check + 自动重试机制。

**第四阶段：章节预处理引入 (当前)**

为彻底解决章节归属不准的问题（MinerU 的 `text_level` 字段在不同文档中表现不一致），引入了创新的"双 LLM 流水线"架构：在题目抽取之前，增加独立的 `ChapterPreprocess` 算子，使用长上下文 LLM 通读全文构建章节映射表。初步测试暴露了章节预处理本身的可靠性问题（详见附录 F），这是下一阶段需要重点解决的核心挑战。

### 1.3 产品目标

| 目标类型 | 目标描述 | 核心衡量指标 (KPI) | 目标值 |
| :--- | :--- | :--- | :--- |
| **核心目标** | 题目提取完整性 | 提取完整率（基于 MinerU 解析结果，而非原书） | > 99% |
| **核心目标** | 章节归属准确性 | 章节覆盖率（有章节标题的题目占比） | > 99% |
| | | 章节准确率（章节路径与人工标注一致的题目占比） | > 95% |
| **工程目标** | 流水线鲁棒性 | LLM 输出有效率（成功解析并验证通过的 LLM 输出占比） | > 98% |
| **商业目标** | 降低人工成本 | 人工干预率（需要人工修正的题目占比） | < 1% |

> **关于"提取完整率"的说明**: 该指标衡量的是"MinerU 已正确解析出的题目中，本系统成功提取的比例"。MinerU 本身的 OCR 错误或遗漏不计入本系统的失败。换言之，本系统的输入上限由 MinerU 的输出质量决定。

---

## 2. 目标用户与使用场景

### 2.1 核心用户原型

**姓名**: 张伟 (David Zhang)

**角色**: AI 公司数据工程师

**背景**: 32 岁，计算机科学硕士，在一家专注于 K12 教育的 AI 创业公司工作 3 年。他负责构建和维护公司的数据处理流水线，为下游的"AI 错题本"、"个性化推荐"等产品提供数据支持。精通 Python 和 SQL，熟悉 Docker、Kubernetes 和主流云服务。日常使用 Airflow 或类似工具编排数据任务。对 LLM 有基本了解，能够通过 API 调用模型，但不是算法专家。

**痛点**:

> "每周，市场部都会给我们上百份 PDF 格式的各科新教辅材料。我的任务就是把里面的题目都扒出来，放到我们的题库里。之前用开源的 OCR 工具，版面稍微一变就全乱了。后来尝试用脚本加正则表达式，结果每个学科、每个出版社都要写一套规则，维护成本高到爆炸。最头疼的是，业务方要求题目必须带着它所属的章节，比如'第一章 > 第二节 > 函数的单调性'，这样才能做知识点分析。现在的工具要么提不出来，要么提出来的都是错的，我每周至少要花一天时间手动去修正这些章节信息，简直是噩梦。"

**目标**:
1.  **自动化**: 找到一个能自动、准确地从 PDF 中提取题目和章节信息的工具，将他从繁琐的手工劳动中解放出来。
2.  **高准确性**: 提取结果的准确率要足够高（人工干预率 < 1%），特别是章节归属关系，以减少下游数据清洗的工作量。
3.  **可观测与可追溯**: 当提取出错时，他需要能快速定位问题所在——是 OCR 错了？还是 LLM 理解错了？或是哪个环节的配置有问题？他需要清晰的日志和中间产物来帮助他诊断和修复。
4.  **易于集成**: 工具最好能提供稳定的 API，方便他集成到现有的数据流水线中。

### 2.2 用户故事

| ID | 作为... | 我希望... | 以便... | 优先级 |
| :--- | :--- | :--- | :--- | :--- |
| US-000 | 数据工程师 | 将大量的教育文本 PDF（数学课本、讲义、试卷、练习等）通过 MinerU 解析成包含 `content_list.json`、Markdown、`images/` 等标准输出物 | 供本系统的提取任务使用和加工处理 | **前置条件** |
| US-001 | 数据工程师 | 上传一个包含 `content_list.json` 和 `images/` 的 MinerU 输出文件夹，就能启动一个完整的提取任务 | 快速开始工作，而不需要复杂的预处理 | **必须有** |
| US-002 | 数据工程师 | 在一个统一的界面管理所有 LLM 的 API Key 和模型配置 | 方便地测试和切换不同的模型，找到性价比最高的方案 | **必须有** |
| US-003 | 数据工程师 | 实时查看任务的处理进度、分块状态和详细日志 | 及时了解任务健康状况，在出现问题时第一时间介入 | **必须有** |
| US-004 | 数据工程师 | 在任务完成后，能方便地预览和下载结构化的结果 (JSON/Markdown) | 快速验证提取质量，并将数据交付给下游团队 | **必须有** |
| US-005 | 数据工程师 | 当任务失败或结果不理想时，能一键重跑任务，并对比新旧两个版本的结果差异 | 高效地调试 Prompt 或配置，并直观地评估优化效果 | **应该有** |
| US-006 | 数据工程师 | 系统能自动检测并修复 LLM 输出的常见错误（如 JSON 格式、ID 错误） | 提高流水线的成功率，减少因 LLM 偶尔"发疯"导致的任务失败 | **必须有** |

> **关于 US-000 的说明**: MinerU 解析是本系统的**外部前置依赖**，不属于本系统的功能范围。用户需要自行完成 MinerU 解析，并将输出物上传到本系统。本系统不重新执行 OCR，也不替代 MinerU 的解析链路。

---

## 3. 产品原则

以下原则是 Mineru2Questions 的核心价值观，是指导整个团队在面对设计、技术和业务决策时进行权衡取舍的最高准则。

**原则一：准确性高于一切 (Accuracy Over All)**

在速度、成本和准确性之间权衡时，永远优先选择准确性。一条错误的或结构不完整的数据会给下游系统带来数倍的修复成本。我们宁愿处理得慢一些、贵一些，也要确保交付数据的质量。

**原则二：过程必须可观测 (Observability is a Feature)**

系统不能是一个"黑盒"。用户必须能够清晰地看到数据处理的每一个环节、每一个中间产物和每一条决策日志。良好的可观测性是建立信任和赋能用户自主解决问题的前提。

**原则三：坚持 ID-Only (ID-Only is the Law)**

LLM 的职责是"理解结构"而非"创造内容"。流水线中，LLM 只允许输出对 `content_list.json` 中 Block ID 的引用（ID / ID 区间 / ID 列表）。所有最终文本都必须通过 ID 回填得到。这是保证内容"保真"不可动摇的基石。明确反对：让 LLM 输出自由文本题干/答案再做匹配。

**原则四：拥抱失败，优雅降级 (Embrace Failure, Degrade Gracefully)**

我们必须承认，无论是 OCR 还是 LLM，在可预见的未来都会犯错。系统设计必须将"失败"作为一等公民来考虑，在每个环节都建立验证、重试、告警和回退机制，确保局部失败不会导致整个任务的灾难性崩溃。**特别地，当章节预处理失败时，系统必须能够回退到使用题目抽取阶段自带的章节信息，而非输出错误的章节映射。**

**原则五：不靠硬编码，追求可泛化 (Generalize, Don't Hardcode)**

系统必须能够兼容多种教育文本（不同学科、不同排版、不同题型、不同来源）。明确反对通过"硬编码 if/else + 特例补丁"来追求短期效果。所有规则和策略应以可泛化的信号为基础（结构、位置、Block 类型、编号模式、语义边界、页面连续性等），并提供可配置、可扩展、可回退的机制。

---

## 4. 功能需求：前端与用户交互 (UI/UX)

本系统采用单页应用 (SPA) 架构，通过左侧导航栏 + 右侧内容区的 Dashboard 布局组织所有页面。

### 4.1 整体布局 (`DashboardLayout`)

**目的**: 提供一致的导航和页面布局框架。

**组件与交互**:
- 左侧为固定的垂直导航栏，包含以下入口：
  - **首页** (`/`): 系统概览仪表盘
  - **任务列表** (`/tasks`): 所有任务的管理中心
  - **新建任务** (`/tasks/new`): 创建新的提取任务
  - **设置** (`/settings`): LLM 配置管理
- 导航栏底部显示当前登录用户信息和登出按钮。
- 右侧为内容展示区，根据路由动态渲染对应页面。
- 导航栏当前激活的菜单项以高亮样式标识。

### 4.2 首页 (`Home`)

**目的**: 提供系统概览和快速入口。

**组件与交互**:
- **统计卡片区域**: 以卡片形式展示关键指标，包括"进行中的任务数"、"已完成的任务数"、"总提取题目数"。每张卡片包含图标、数值和标签。
- **最近任务列表**: 展示最近创建或更新的 5 个任务，每个条目显示任务名称、状态徽标（使用不同颜色区分：蓝色=处理中、绿色=已完成、红色=失败、灰色=排队中）和创建时间，点击可跳转到任务详情。
- **快速操作按钮**: 提供"新建任务"的醒目按钮，点击跳转到 `NewTask` 页面。
- **空状态**: 当没有任何任务时，显示引导文案和"创建第一个任务"的按钮。

### 4.3 新建任务 (`NewTask`)

**目的**: 引导用户完成新提取任务的创建。

**组件与交互**:
- **任务名称输入框**: 必填字段，用于标识任务（如"人教版高一数学必修一"）。
- **文件上传区**:
  - 支持拖拽或点击上传。
  - 用户上传的是一个文件夹，该文件夹必须包含 `content_list.json` 文件和 `images/` 子目录（MinerU 的标准输出结构）。
  - 上传过程中显示进度条和文件计数。
  - 上传后，系统自动校验文件夹结构的完整性，并显示文件数量和总大小。
  - 如果缺少 `content_list.json`，显示红色错误提示并阻止提交。
  - 上传成功后，显示绿色确认信息，包含 `content_list.json` 中的 Block 数量和图片数量。
- **LLM 配置选择**:
  - 提供两个独立的下拉选择框：
    - **"章节预处理模型"**: 仅显示 `purpose` 为 `long_context` 或 `general` 的已配置 LLM。用于第一阶段的章节识别，需要大上下文窗口。
    - **"题目抽取模型"**: 仅显示 `purpose` 为 `vision_extract` 或 `general` 的已配置 LLM。用于第二阶段的题目抽取。
  - 如果用户尚未配置任何 LLM，显示黄色提示信息"请先在设置页面配置 LLM"，并提供跳转到"设置"页面的链接。
  - 每个下拉框旁显示所选模型的关键参数（上下文窗口、并发数）。
- **"开始任务"按钮**: 点击后提交表单，创建任务记录，自动启动处理流程，并跳转到任务详情页。按钮在提交过程中显示加载状态，防止重复提交。

### 4.4 任务列表 (`Tasks`)

**目的**: 集中管理所有历史和进行中的任务。

**组件与交互**:
- **任务卡片列表**: 每个任务以卡片形式展示，包含：
  - 任务名称（粗体）
  - 状态徽标（排队中 `pending` / 处理中 `processing` / 已完成 `completed` / 失败 `failed` / 已暂停 `paused`），使用不同颜色和图标区分
  - 进度信息（仅在"处理中"状态显示，格式为"已处理 X/Y 块"）
  - 创建时间（相对时间，如"2 小时前"）
  - 提取题目数（仅在"已完成"状态显示）
- **操作按钮**: 每个卡片提供以下操作（根据状态动态显示/隐藏）：
  - **"查看详情"**: 始终可用，跳转到 `TaskDetail` 页面
  - **"开始"**: 仅在 `pending` 或 `paused` 状态可用
  - **"暂停"**: 仅在 `processing` 状态可用
  - **"重试"**: 在 `completed` 或 `failed` 状态可用，创建一个新的子任务，继承父任务的配置和文件，形成"任务族系"
  - **"删除"**: 始终可用，点击后弹出二次确认对话框，确认后删除任务及其所有关联数据（包括文件）
- **筛选与排序**: 支持按任务状态筛选（全部/进行中/已完成/失败），默认按创建时间倒序排列。
- **空状态**: 当没有任何任务时，显示引导文案和"创建第一个任务"的按钮。

### 4.5 任务详情 (`TaskDetail`)

**目的**: 提供任务处理的实时监控、结果展示和调试支持。这是系统中信息最密集的页面。

**组件与交互**:

**A. 任务概览区** (页面顶部):
- 显示任务名称（大标题）、状态徽标、创建时间、开始时间、完成时间。
- 显示关联的 LLM 配置名称（章节预处理模型 + 题目抽取模型）。
- 显示提取的题目总数（大字号突出显示）。
- 如果任务属于某个"族系"（有父任务或子任务），显示族系标识和版本号。

**B. 操作按钮区** (概览区右侧或下方):
- **"下载结果"**: 仅在 `completed` 状态可用，下载 `questions.json` 和 `questions.md`。
- **"下载调试包"**: 仅在 `completed` 或 `failed` 状态可用，打包下载任务目录下的所有文件（包括 `debug/` 和 `logs/` 目录）。
- **"重试"**: 在 `completed` 或 `failed` 状态可用，创建新的子任务。
- **"暂停"/"继续"**: 仅在 `processing` 状态可用。

**C. 实时日志区** (中部):
- 以类终端样式（深色背景、等宽字体）的滚动窗口展示后端处理日志。
- 日志按时间倒序排列，每条日志显示：
  - 时间戳（精确到秒）
  - 级别标签（`INFO` 绿色 / `WARN` 黄色 / `ERROR` 红色 / `DEBUG` 灰色）
  - 阶段标签（`loading` / `chunking` / `chapter_preprocess` / `extracting` / `merging` / `saving`）
  - 消息内容
- 支持自动滚动到最新日志（默认开启）和手动暂停滚动。
- 提供手动"刷新日志"按钮。
- 在 `processing` 状态下自动轮询（每 3 秒）获取最新日志。

**D. 分块进度区** (日志区下方):
- 以网格形式可视化展示所有数据块（Chunk）的处理状态。
- 每个块显示为一个小方块，颜色表示状态：灰色（`pending` 待处理）、蓝色脉冲（`processing` 处理中）、绿色（`completed` 成功）、红色（`failed` 失败）。
- 鼠标悬停在方块上时，显示 Tooltip，包含 Chunk 索引、Block ID 范围、处理耗时等信息。

**E. 结果预览区** (仅在 `completed` 状态显示):
- 提供两个 Tab 页：**Markdown** 和 **JSON**。
- **Markdown Tab**: 将 `questions.md` 的内容渲染为格式化的文档，支持章节折叠/展开。
- **JSON Tab**: 将 `questions.json` 的内容以语法高亮的方式展示，支持搜索和折叠。

**F. 历史版本区** (仅在任务有父任务或子任务时显示):
- 显示当前任务所属"族系"中的所有版本列表，每个版本显示版本号、状态、题目数和创建时间。
- 提供"对比"按钮，选择两个版本后跳转到 `TaskCompare` 页面。

### 4.6 任务对比 (`TaskCompare`)

**目的**: 直观对比两个任务版本的结果差异，帮助用户评估 Prompt 或配置调整的效果。

**组件与交互**:
- **版本选择区**: 两个下拉框，分别用于选择"基准版本"和"对比版本"，默认选择最近的两个版本。
- **统计对比**: 顶部显示两个版本的关键指标对比（题目总数、有答案的题目数、有章节的题目数），差异以颜色标识（增加=绿色，减少=红色）。
- **差异视图**: 以分屏、差异高亮（类似 Git Diff）的方式，展示两个版本 Markdown 输出的逐行差异。新增内容以绿色高亮，删除内容以红色高亮，修改内容以黄色高亮。

### 4.7 设置 (`Settings`)

**目的**: 统一管理所有外部 LLM 服务的 API 配置。

**组件与交互**:

**A. 配置列表**:
- 以表格形式展示所有已添加的 LLM 配置，每行显示：
  - 配置名称
  - 提供商图标和名称
  - 模型名称
  - 用途标签（`视觉抽取` / `长文本推理` / `通用`，使用不同颜色的 Badge）
  - 并发数
  - 是否为默认（星标图标）
  - 操作按钮：编辑（铅笔图标）、删除（垃圾桶图标）
- 表格上方有"新增配置"按钮。

**B. 新建/编辑模态框**:
- **预设提供商选择**: 下拉框，可选择"SiliconFlow (硅基流动)"、"OpenAI"、"Anthropic Claude"、"Google Gemini"、"DeepSeek"或"自定义 API"。选择预设后，自动填充 API URL 和默认模型名称列表。
- **模型选择**: 根据选定的提供商，显示该提供商支持的模型列表供选择（下拉框）。每个模型选项显示模型名称、上下文窗口大小和推荐用途。选择"自定义 API"时，此字段变为自由输入框。
- **配置字段**:
  - **配置名称** (必填): 用户自定义的名称，如"硅基流动-Qwen-VL"
  - **API URL** (必填): 预设自动填充，也可手动修改
  - **API Key** (必填): 用户输入，界面上以密码形式显示（带显示/隐藏切换按钮）
  - **模型名称** (必填): 预设自动填充，也可手动输入
  - **用途** (必填): 下拉选择"视觉抽取"(`vision_extract`)、"长文本推理"(`long_context`) 或"通用"(`general`)。该字段决定此配置在 NewTask 页面中出现在哪个下拉框中。
  - **并发数** (选填): 默认 5，范围 1-50，带数字输入框和滑块
  - **超时时间** (选填): 默认 300 秒，范围 30-1800 秒
  - **上下文窗口** (选填): 默认 128000 tokens，选择预设模型时自动填充
  - **设为默认** (选填): 勾选后，该配置将成为同用途下的默认选项。同一用途下只能有一个默认配置，设置新默认时自动取消旧默认。
- **"测试连接"按钮**: 在保存前，向 LLM API 发送一条简单的测试消息（如"Hello"），验证 API Key 和模型的连通性。测试过程中按钮显示加载状态。成功显示绿色提示（含模型返回的响应片段），失败显示红色错误信息（含具体的 HTTP 状态码或错误描述）。
- **"保存"按钮**: 验证所有必填字段后保存配置。保存成功后自动关闭模态框并刷新列表。
- **"取消"按钮**: 关闭模态框，不保存任何更改。


---

## 5. 功能需求：后端核心流水线

### 5.1 流水线总览

本系统的核心是一条由 7 个算子组成的"双 LLM 流水线"。之所以称为"双 LLM"，是因为它在两个不同阶段使用了两种不同类型的 LLM：第一阶段使用**长上下文 LLM** 进行全局章节识别，第二阶段使用**视觉/通用 LLM** 进行分块题目抽取。

> **与官方 DataFlow 流水线的关键差异**: 官方 `PDF_VQA_extract_optimized_pipeline` 没有独立的章节预处理算子，章节识别是在题目抽取阶段由 LLM 同步完成的。本项目之所以引入独立的章节预处理，是因为在实践中发现：(1) MinerU 的 `text_level` 字段在不同文档中表现不一致，不能可靠地用于判断标题层级；(2) 分块处理时，章节标题可能不在当前 Chunk 中，导致 LLM 无法正确识别。独立的章节预处理算子使用长上下文 LLM 通读全文，从全局视角构建章节映射，理论上能解决这两个问题。

**流水线算子顺序**:

```
[输入: content_list.json + images/]
        │
        ▼
┌─────────────────────────┐
│ ① BlockFlattener        │ → FlatBlock[]
└─────────┬───────────────┘
          │
          ▼
┌─────────────────────────┐
│ ② ChapterPreprocess     │ → 章节候选 JSON (两轮 LLM)
│    (长上下文 LLM)        │
└─────────┬───────────────┘
          │
          ▼
┌─────────────────────────┐
│ ③ ChapterValidation     │ → chapter_flat_map.json | null
│    (验证与容错)          │
└─────────┬───────────────┘
          │
          ▼
┌─────────────────────────┐
│ ④ QuestionExtract       │ → XML 输出 (含 chapter_title)
│    (视觉/通用 LLM)      │
└─────────┬───────────────┘
          │
          ▼
┌─────────────────────────┐
│ ⑤ Parser                │ → ExtractedQuestion[]
│    (ID 回填)             │
└─────────┬───────────────┘
          │
          ▼
┌─────────────────────────┐
│ ⑥ ChapterMerge          │ → 融合后的 ExtractedQuestion[]
│    (章节信息融合)        │
└─────────┬───────────────┘
          │
          ▼
┌─────────────────────────┐
│ ⑦ PostProcess & Export   │ → questions.json + questions.md
│    (去重、排序、导出)    │
└─────────┘

[输出: questions.json + questions.md]
```

> **关键设计决策 — ChapterMerge 而非 ChapterOverwrite**: 在早期版本中，算子 ⑥ 被设计为"ChapterOverwrite"（强制覆盖），即章节预处理的结果无条件覆盖题目抽取阶段的章节信息。测试发现，当章节预处理失败时，这种策略会把正确的章节信息替换为错误的。因此，v2.0 将其修订为"ChapterMerge"（融合），即同时保留两个来源的章节信息，并根据可靠性进行融合决策。当章节预处理完全失败（`chapter_flat_map` 为 `null`）时，系统回退到仅使用题目抽取阶段的章节信息。

### 5.2 算子 ①：BlockFlattener

**职责**: 将 MinerU 输出的嵌套 `content_list.json` 展平为一维的 `FlatBlock[]` 数组，为后续所有算子提供统一的、连续编号的输入。

**输入**: MinerU 原始输出的 `content_list.json`。该文件是一个 JSON 数组，每个元素代表一个"Block"（段落、公式、图片、表格、列表等），可能包含嵌套结构（如 `list` 类型包含子项）。

**输出**: `FlatBlock[]` 数组，每个元素具有唯一的、从 0 开始连续递增的 `id`。同时将结果写入 `debug/formatted_blocks.json`。

**核心规则**:
- 嵌套的 `list` 类型必须递归展平为独立的 Block，每个子项获得独立的 `id`。
- `equation` 类型的 `latex` 字段必须被保留并正确转换为文本表示。
- 噪声 Block（如页码、页眉页脚等无意义内容）应被过滤，但过滤规则必须是保守的（宁可保留噪声，不可误删有效内容）。
- 每个 `FlatBlock` 必须保留原始的 `page_idx`（页码）和 `category_type`（Block 类型）信息。
- `id` 的连续性是整个流水线的基石——后续所有算子的 ID 引用都基于此。

**验收标准**: 输出的 `FlatBlock[]` 的 `id` 必须从 0 开始连续递增，无间断。所有非噪声的原始 Block 内容都必须被保留。

### 5.3 算子 ②：ChapterPreprocess

**职责**: 使用长上下文 LLM 通读全文的 Block 列表，识别出所有章节标题及其对应的 Block ID，构建全局章节结构。

**输入**: `FlatBlock[]` 数组（来自算子 ①）。

**输出**: 章节候选 JSON 数组，每个元素包含 `block_id`（该章节标题对应的 Block ID）、`title`（章节标题文本）、`level`（层级：1=章、2=节、3=小节等）。

**核心规则**:
- **必须使用两轮 LLM 调用**: 第一轮进行初步识别，第二轮对第一轮的结果进行校验和修正。两轮使用相同的 LLM 配置。
- LLM 的输出必须是结构化的 JSON，只包含 Block ID 引用和标题文本，不包含题目内容。
- 当文档的 Block 数量超过 LLM 上下文窗口限制时，必须自动切换到分块模式，将 Block 列表分成多个 Chunk 分别处理，然后合并结果。
- 分块模式下，必须确保每个 Chunk 的 LLM 输出中的 Block ID 是原始 `FlatBlock[]` 中的真实 ID，而非 Chunk 内部的相对序号。
- 必须将 LLM 的原始输出保存到 `debug/` 目录，包括每一轮的 Prompt 和 Response。

**验收标准**: 输出的每个 `block_id` 必须在 `FlatBlock[]` 的 ID 范围内。章节标题的顺序必须与文档中的出现顺序一致。

### 5.4 算子 ③：ChapterValidation

**职责**: 验证 `ChapterPreprocess` 的输出质量，构建最终的章节映射表 (`chapter_flat_map`)，或在验证失败时输出 `null` 以触发回退。

**输入**: 章节候选 JSON（来自算子 ②）+ `FlatBlock[]`（来自算子 ①）。

**输出**: `chapter_flat_map.json`（章节映射表，每个条目包含章节标题和其覆盖的 Block ID 范围）或 `null`（表示验证失败，应回退到题目抽取阶段的章节信息）。

**核心规则**:
- 必须验证以下条件：
  - JSON 格式合规（可解析为预期结构）
  - 所有 `block_id` 在合法范围内
  - 章节覆盖的 Block ID 范围逻辑合理（无大面积空白、无严重重叠）
- 当验证失败时，必须输出 `null` 而非使用错误的数据。这是"优雅降级"原则的体现。
- 验证结果（通过/失败及原因）必须记录到任务日志中。

**验收标准**: 当章节预处理的 LLM 输出存在格式错误、ID 越界或逻辑异常时，该算子必须能够检测到并输出 `null`，而非让错误数据流入下游。

### 5.5 算子 ④：QuestionExtract

**职责**: 使用视觉/通用 LLM 对 `FlatBlock[]` 进行分块处理，从每个 Chunk 中抽取题目结构。

**输入**: `FlatBlock[]` 数组（来自算子 ①）+ 关联的图片文件。

**输出**: 每个 Chunk 的 LLM 原始输出（XML 格式文本），包含题目的 Block ID 引用和 LLM 自行判断的章节标题。

**核心规则**:
- **ID-Only 原则**: LLM 只允许输出 Block ID 引用（`<question_ids>`、`<answer_ids>` 等），不允许输出自由文本题干或答案。所有最终文本通过 ID 回填获得。
- **分块策略**: 将 `FlatBlock[]` 按固定数量（如每 Chunk 包含 N 个 Block）分块。每个 Chunk 的 Block 必须保持连续性，不可跨页截断。
- **并发控制**: 多个 Chunk 应通过并发池并行处理，并发数由 LLM 配置的 `maxWorkers` 参数控制。
- **Sanity Check + 重试**: 每个 Chunk 的 LLM 输出必须经过基本的合理性检查（如输出不为空、包含预期的 XML 标签）。不通过检查的 Chunk 应自动重试（最多重试 N 次）。
- **LLM 注意力衰减应对**: 当某个 Chunk 的输出题目数异常少时（远低于同文档其他 Chunk 的平均值），应触发重试。
- 每个 Chunk 的 Prompt 和 LLM 原始输出必须保存到 `debug/` 和 `logs/` 目录。

**验收标准**: 基于 MinerU 已正确解析的题目，提取完整率 > 99%。LLM 输出有效率 > 98%。

### 5.6 算子 ⑤：Parser

**职责**: 解析 LLM 的 XML 输出，通过 ID 回填从 `FlatBlock[]` 中还原题目的完整文本，生成结构化的 `ExtractedQuestion[]`。

**输入**: LLM 原始 XML 输出（来自算子 ④）+ `FlatBlock[]`（来自算子 ①）。

**输出**: `ExtractedQuestion[]` 数组，每个元素包含完整的题目文本、Block ID 列表、页码、LLM 判断的章节标题等。

**核心规则**:
- **双模式解析**: 先尝试严格模式（要求 XML 完全合规），失败后回退到宽松模式（使用正则表达式提取关键字段）。
- **ID 回填**: 将 LLM 输出的 Block ID 列表映射回 `FlatBlock[]`，拼接对应 Block 的文本内容，形成完整的题目文本。
- 必须保留 LLM 输出的 `chapter_title` 字段（即使后续可能被 ChapterMerge 覆盖），因为它是 ChapterMerge 的输入之一。
- 解析失败的题目应记录到日志中，但不应导致整个 Chunk 的解析失败。

**验收标准**: 对于格式正确的 LLM 输出，解析成功率必须为 100%。对于格式有轻微错误的输出，宽松模式应能恢复大部分题目。

### 5.7 算子 ⑥：ChapterMerge

**职责**: 融合两个来源的章节信息——`chapter_flat_map`（来自算子 ③）和每道题目自带的 `chapter_title`（来自算子 ⑤），为每道题目确定最终的章节归属。

**输入**: `ExtractedQuestion[]`（来自算子 ⑤）+ `chapter_flat_map.json`（来自算子 ③，可能为 `null`）。

**输出**: 章节信息已确定的 `ExtractedQuestion[]`。

**核心规则**:
- **当 `chapter_flat_map` 为 `null` 时**: 直接使用每道题目自带的 `chapter_title`（来自题目抽取阶段的 LLM 输出），不做任何覆盖。这是回退模式。
- **当 `chapter_flat_map` 可用时**: 根据每道题目的 Block ID，在 `chapter_flat_map` 中查找其所属的章节。融合策略应综合考虑两个来源的信息，而非简单地用一个覆盖另一个。
- 融合后的章节标题应尽可能保持一致性（同一章节下的题目应有相同的章节标题）。
- 必须记录每道题目的章节来源（"来自章节预处理"或"来自题目抽取"或"融合"），以便调试。

**验收标准**: 当章节预处理成功时，章节覆盖率 > 99%，章节准确率 > 95%。当章节预处理失败时，系统必须正常回退，不输出错误的章节信息。

### 5.8 算子 ⑦：PostProcess & Export

**职责**: 对所有题目进行去重、排序和格式化，生成最终的交付文件。

**输入**: 章节信息已确定的 `ExtractedQuestion[]`（来自算子 ⑥）。

**输出**: `results/questions.json`（结构化 JSON）+ `results/questions.md`（可读 Markdown）。

**核心规则**:
- **去重**: 基于 `questionIds`（Block ID 序列）进行精确去重。这是经过多次迭代验证的最可靠去重方式。明确不使用 `(title, label)` 或 `(title, label, page_idx)` 等不稳定的去重键。
- **排序**: 按页码 (`page_idx`) 和 Block ID 升序排列。
- **Markdown 格式**: 按章节分组，每个章节下列出所有题目。题目包含编号、题干文本、关联图片（如有）和答案/解题过程（如有）。
- **JSON 格式**: 每道题目包含完整的结构化字段（详见附录 A 的 `ExtractedQuestion` 类型定义）。

**验收标准**: 输出文件中不存在重复题目。所有题目按文档顺序排列。JSON 和 Markdown 的内容一致。

---

## 6. 系统架构与技术栈

### 6.1 架构概览

系统采用前后端分离的单体应用架构，前端通过 tRPC 与后端通信，后端通过 OpenAI 兼容 API 与外部 LLM 服务通信。

```
┌─────────────────────────────────────────────────────┐
│                     前端 (React SPA)                 │
│  Vite + React + TypeScript + TailwindCSS + shadcn   │
│  Wouter (路由) + @tanstack/react-query (数据获取)    │
└──────────────────────┬──────────────────────────────┘
                       │ tRPC (HTTP/WebSocket)
                       ▼
┌─────────────────────────────────────────────────────┐
│                     后端 (Node.js)                   │
│  Hono (HTTP) + tRPC (API) + Drizzle ORM (数据库)     │
│                                                      │
│  ┌─────────────────────────────────────────────┐    │
│  │           核心流水线 (7 个算子)               │    │
│  │  BlockFlattener → ChapterPreprocess →        │    │
│  │  ChapterValidation → QuestionExtract →       │    │
│  │  Parser → ChapterMerge → PostProcess         │    │
│  └─────────────────────────────────────────────┘    │
└──────────┬──────────────────────┬───────────────────┘
           │                      │
           ▼                      ▼
┌──────────────────┐   ┌──────────────────────────┐
│  SQLite 数据库    │   │  外部 LLM 服务            │
│  (Drizzle ORM)   │   │  (OpenAI 兼容 API)       │
│  本地文件存储     │   │  SiliconFlow / OpenAI /  │
└──────────────────┘   │  Anthropic / Google /    │
                       │  DeepSeek / 自定义        │
                       └──────────────────────────┘
```

### 6.2 技术栈约束

| 层级 | 技术选型 | 版本要求 | 说明 |
| :--- | :--- | :--- | :--- |
| **运行时** | Node.js | >= 22.x | 后端运行时 |
| **语言** | TypeScript | >= 5.x | 前后端统一语言，严格模式 |
| **前端框架** | React | >= 18.x | UI 框架 |
| **前端构建** | Vite | >= 5.x | 开发服务器和打包工具 |
| **前端路由** | Wouter | >= 3.x | 轻量级客户端路由 |
| **前端数据获取** | @tanstack/react-query | >= 5.x | 服务端状态管理 |
| **UI 组件库** | shadcn/ui + Radix UI | 最新 | 基于 TailwindCSS 的组件库 |
| **CSS 框架** | TailwindCSS | >= 3.x | 原子化 CSS |
| **后端 HTTP 框架** | Hono | >= 4.x | 轻量级 HTTP 框架 |
| **API 层** | tRPC | >= 11.x | 类型安全的 API 层 |
| **数据库** | SQLite | - | 嵌入式数据库，无需独立部署 |
| **ORM** | Drizzle ORM | >= 0.38.x | TypeScript-first 的 ORM |
| **LLM 客户端** | OpenAI SDK | >= 4.x | 兼容所有 OpenAI API 规范的 LLM 服务 |
| **包管理器** | npm | >= 10.x | 依赖管理 |

**硬性约束**:
- **不引入 Python 依赖**: 整个项目必须在纯 Node.js/TypeScript 环境下运行。
- **不重新执行 OCR**: 系统信任并基于 MinerU 的输出，不替代 MinerU 的解析链路。
- **LLM 通信协议**: 所有 LLM 调用必须通过 OpenAI 兼容的 `/chat/completions` API 端点进行，以确保对多个提供商的兼容性。


---

## 7. 发布标准

以下是系统正式发布前必须满足的最低标准。

### 7.1 性能

| 指标 | 标准 | 测试条件 |
| :--- | :--- | :--- |
| 端到端处理时间 | **不超过 30 分钟** | 1500 页 PDF，包含约 5000 道数学题目，标准 LLM 配置（并发数 5，超时 300 秒） |
| 前端页面首次加载 | < 3 秒 | 标准网络环境 |
| 任务列表查询响应 | < 500 毫秒 | 数据库中有 100 个任务 |
| 日志实时推送延迟 | < 5 秒 | 从后端产生日志到前端显示 |

### 7.2 可靠性

| 指标 | 标准 | 说明 |
| :--- | :--- | :--- |
| 题目提取完整率 | > 99% | 基于 MinerU 已正确解析的题目 |
| 章节覆盖率 | > 99% | 有章节标题的题目占比（章节预处理成功时） |
| 章节准确率 | > 95% | 章节路径与人工标注一致的题目占比 |
| LLM 输出有效率 | > 98% | 成功解析并验证通过的 LLM 输出占比 |
| 人工干预率 | < 1% | 需要人工修正的题目占比 |
| 系统内部错误失败率 | < 1% | 非 LLM/网络原因导致的任务失败 |

### 7.3 可用性

| 指标 | 标准 | 说明 |
| :--- | :--- | :--- |
| 新用户完成首次任务 | < 10 分钟 | 从注册到看到第一个任务的结果（不含 MinerU 解析时间） |
| 任务失败时的诊断时间 | < 5 分钟 | 用户能通过日志和调试文件定位问题根因 |

### 7.4 可维护性

| 指标 | 标准 | 说明 |
| :--- | :--- | :--- |
| 代码测试覆盖率 | > 70% | 核心流水线算子的单元测试覆盖率 |
| 中间产物完整性 | 100% | 每个算子的输入输出都必须有对应的调试文件 |

---

## 8. 开发路线图

以下路线图基于当前项目状态和已知问题，按优先级排列。

### 8.1 P0 — 必须立即解决（阻塞核心功能）

| 编号 | 任务 | 说明 | 关联用户故事 |
| :--- | :--- | :--- | :--- |
| P0-1 | **实现 ChapterValidation 算子** | 当前代码中缺少对章节预处理 LLM 输出的验证。必须实现 JSON 格式验证、Block ID 合法性检查和逻辑合理性检查。验证失败时输出 `null`。 | US-006 |
| P0-2 | **将 ChapterOverwrite 改为 ChapterMerge** | 当前的"强制覆盖"策略在章节预处理失败时会导致所有题目的章节信息错误。必须改为融合策略，支持回退。 | US-001, US-006 |
| P0-3 | **修复分块模式下的 ID 空间问题** | 章节预处理在分块模式下，LLM 可能输出 Chunk 内部的相对序号而非全局 Block ID。必须在 Prompt 或后处理中确保 ID 的正确性。 | US-006 |

### 8.2 P1 — 应该尽快解决（影响数据质量）

| 编号 | 任务 | 说明 | 关联用户故事 |
| :--- | :--- | :--- | :--- |
| P1-1 | **增强章节预处理对目录页的处理** | 测试中观测到 LLM 可能将目录页的条目误识别为正文中的章节标题。需要在 Prompt 或后处理中增强对目录页的区分能力。 | US-001 |
| P1-2 | **完善 Sanity Check 策略** | 扩展 Sanity Check 的覆盖范围，不仅检查题目数量，还应检查 Block ID 的覆盖率和连续性。 | US-006 |
| P1-3 | **任务对比功能完善** | 当前 TaskCompare 页面的差异算法需要优化，确保能准确展示两个版本之间的增删改差异。 | US-005 |

### 8.3 P2 — 可以后续优化（提升体验和可维护性）

| 编号 | 任务 | 说明 | 关联用户故事 |
| :--- | :--- | :--- | :--- |
| P2-1 | **增加单元测试** | 为核心流水线的每个算子编写单元测试，覆盖正常路径和异常路径。 | - |
| P2-2 | **History 页面与 Tasks 页面去重** | 明确两个页面的差异化定位，或考虑合并。 | US-003 |
| P2-3 | **探索远距离答案匹配** | 作为未来版本的核心议题，研究如何将书末答案与正文题目进行可靠配对。 | - |
| P2-4 | **支持批量任务** | 允许用户一次上传多个 MinerU 输出文件夹，批量创建和执行任务。 | US-001 |

