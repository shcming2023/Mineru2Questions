# Mineru2Questions 项目技术审查 - 执行摘要

## 项目背景

**项目名称**: Mineru2Questions  
**项目地址**: https://github.com/shcming2023/Mineru2Questions  
**参考标准**: OpenDCAI/DataFlow (https://github.com/OpenDCAI/DataFlow)  
**测试任务**: https://github.com/shcming2023/Mineru2Questions/tree/main/server/uploads/tasks/202602080714-1770506098605  
**审查日期**: 2026-02-08  

Mineru2Questions 是一个基于 MinerU 解析结果进行题目与问答对抽取的项目,采用 TypeScript/Node.js/SQLite 技术栈。该项目的核心目标是从教育类 PDF 文档中自动提取结构化的题目、答案和解答过程,支持问题和答案分离的教材场景。

## 审查方法

本次审查以 OpenDCAI/DataFlow 仓库中的 `PDF_VQA_extract_optimized_pipeline` 及相关算子设计、输入输出约定、容错机制作为唯一事实标准 (Source of Truth),对 Mineru2Questions 项目的当前实现进行逐算子级别的对齐审查。审查覆盖以下六个核心阶段:

1. **输入格式化与标准化** - 对应官方 `MinerU2LLMInputOperator`
2. **ID 列表构建与候选区域筛选** - 对应官方 `ChunkedPromptedGenerator` 的 chunking 逻辑
3. **基于上下文的 LLM 抽取** - 对应官方 `ChunkedPromptedGenerator` 和 `QAExtractPrompt`
4. **ID 回填原文** - 对应官方 `LLMOutputParser` 的 `_id_to_text` 方法
5. **问答对合并与去重** - 对应官方 `QA_Merger` 和 `merge_qa_pair` 函数
6. **质量评估与回退策略** - 对应官方的质量过滤和容错机制

## 核心发现

### 整体评估

Mineru2Questions 项目在整体架构上遵循了 OpenDCAI/DataFlow 官方流水线的核心思想,特别是**基于 ID 的逻辑组装方案** (LLM 只输出 ID,文本通过 ID 回填)。这是该项目最重要的设计决策,也是与官方流水线对齐的关键。

然而,在多个关键环节存在偏离、容错缺失和过严约束,导致抽取覆盖率和稳定性不足。

### 对齐情况总览

| 阶段 | 对齐度 | 主要问题 | 优先级 |
|------|--------|----------|--------|
| 1. 输入格式化 | ⚠️ 部分对齐 | `isTocList()` 过严过滤可能误判选项列表 | P0 |
| 2. ID 列表构建 | ✅ 高度对齐 | 已实现 Overlap 窗口,优于官方 | - |
| 3. LLM 抽取 | ✅ 高度对齐 | 提示词增强,重试机制对齐 | - |
| 4. ID 回填 | ✅ 高度对齐 | 逻辑正确,容错处理完善 | - |
| 5. 合并去重 | ⚠️ 部分偏离 | 三层索引复杂,缺失字段级别增量更新 | P0 |
| 6. 质量评估 | ❌ 缺失容错 | 无二次提示、无冲突解决策略 | P0 |

### 测试任务分析

测试任务 (八上数学测试.pdf) 的抽取结果:

- **总题目数**: 225 道
- **章节分布**: 11 个章节 (19.1, 20.1, 20.2, 21.1-21.5, 22.1-22.3)
- **Label 分布**: 存在大量 label=1 的题目 (21 个),说明跨章节题号重置正常
- **章节题目数**: 19.1 章节有 62 道题目,占比最高

从结果来看,当前实现已经能够提取大量题目,但可能存在以下问题:

1. **19.1 章节题目数异常高** (62 道) - 可能存在重复提取或合并错误
2. **部分章节题目数较少** (20.1 和 20.2 各只有 1 道) - 可能存在漏提取
3. **Label 重复** (多个章节都有 label=1) - 说明章节边界检测正常,但需要验证是否有误判

## 关键问题分类

### P0 - 数据丢失风险 (高优先级)

这些问题可能导致题目丢失或错误覆盖,需要立即修复。

#### 问题 1: 输入阶段过严过滤

**位置**: `extraction.ts::convertContentList()` 中的 `isTocList()` 调用

**问题描述**: 在输入格式化阶段使用 `isTocList()` 过滤目录列表,可能误判选项列表 (A. B. C. D.) 为目录,导致选择题选项丢失。

**官方做法**: 不在输入阶段过滤,而是依赖 LLM 提示词和后处理阶段过滤。

**影响范围**: 所有包含选项列表的选择题

**诊断路径**:
1. 检查 `isTocList()` 的日志,查看被过滤的 list 内容
2. 搜索结果中是否存在选择题但缺少选项
3. 对比官方: 官方不在此阶段过滤

**修复方案**: 移除 `convertContentList()` 中的 `isTocList()` 调用,改为在 `parseLLMOutput()` 中使用更精确的后处理过滤。

**预计工作量**: 2 小时

---

#### 问题 2: 缺失字段级别的增量更新

**位置**: `extraction.ts::mergeQAPairs()` 的合并逻辑

**问题描述**: 当同一题目在不同 chunk 中被部分提取 (一次只有 answer,一次只有 solution) 时,当前实现使用 `shouldReplaceQAPair()` 的简单长度比较,可能导致数据丢失。

**官方做法**: 在 answer 阶段进行字段级别的增量更新,只更新缺失的字段。

```python
# DataFlow/dataflow/utils/pdf2vqa/format_utils.py L98-L101
if not answers[(data["chapter_title"], data['label'])].get("solution") and data.get("solution"):
    answers[(data["chapter_title"], data['label'])]["solution"] = data["solution"]
if not answers[(data["chapter_title"], data['label'])].get("answer") and data.get("answer"):
    answers[(data["chapter_title"], data['label'])]["answer"] = data["answer"]
```

**影响范围**: 跨 chunk 的题目 (特别是在 chunk 边界附近的题目)

**诊断路径**:
1. 检查 `questionByIds` 去重日志,查看被替换的题目
2. 对比被替换前后的 question/answer/solution 内容
3. 验证是否存在"一次只有 answer,一次只有 solution"的情况

**修复方案**: 在 `mergeQAPairs()` 中增加字段级别的增量更新逻辑,对齐官方实现。

**预计工作量**: 4 小时

---

#### 问题 3: 无二次提示机制

**位置**: `extraction.ts::callLLMForTextExtraction()` 的调用逻辑

**问题描述**: 当 LLM 返回 `<empty></empty>` 时,直接丢弃该 chunk,可能导致数据丢失。

**官方可能的做法**: 使用二次提示或回退策略 (需进一步验证)。

**影响范围**: 返回空结果的 chunk (可能包含复杂页面或边界情况)

**诊断路径**:
1. 统计返回 `<empty></empty>` 的 chunk 数量
2. 检查这些 chunk 的原始内容是否确实为空
3. 尝试手动调整提示词或增加示例,验证是否能提取

**修复方案**: 实现 `callLLMWithRetry()` 函数,当第一次返回空结果时,使用增强提示词重试。

**预计工作量**: 2 小时

---

### P1 - 稳定性风险 (中优先级)

这些问题可能导致抽取不稳定或匹配失败,建议在短期内修复。

#### 问题 4: 三层索引结构复杂

**位置**: `extraction.ts::mergeQAPairs()` 的索引结构

**问题描述**: 使用三层索引 (`questionByIds`, `questionMapExact`, `questionMapFuzzy`) 偏离官方的简单 `(chapter_title, label)` 索引,增加了复杂度,可能导致匹配逻辑不一致。

**官方做法**: 只使用 `(chapter_title, label)` 作为唯一键。

**影响范围**: 所有问答对的匹配

**诊断路径**:
1. 统计 `questionMapExact` 和 `questionMapFuzzy` 的匹配成功率
2. 检查未匹配的题目的 chapter_title 和 label
3. 对比官方的单层索引逻辑

**修复方案**: 重构 `mergeQAPairs()`,移除三层索引,简化为官方的单层索引 `Map<string, ExtractedQAPair>`,key 为 `${chapterId}:${chapter}:${label}`。

**预计工作量**: 4 小时

---

#### 问题 5: max_tokens 可能不足

**位置**: `extraction.ts::callLLMForTextExtraction()` 的 `max_tokens` 参数

**问题描述**: 固定的 `max_tokens=16384` 无法适应不同 chunk 的题目数量,可能导致输出被截断。

**官方做法**: 未在代码中明确设置,可能使用模型默认值 (需进一步验证)。

**影响范围**: 包含大量题目的 chunk

**诊断路径**:
1. 检查 LLM 原始输出,查看是否存在截断 (最后一个 `</chapter>` 标签不完整)
2. 统计每个 chunk 的输出 token 数量
3. 对比 max_tokens 设置和实际输出长度

**修复方案**: 实现 `calculateMaxTokens()` 函数,根据 chunk 大小动态计算 max_tokens。

**预计工作量**: 2 小时

---

### P2 - 可观测性不足 (低优先级)

这些问题不影响功能,但会增加调试难度,建议在中期改进。

#### 问题 6: 缺少中间产物保存

**位置**: 所有阶段

**问题描述**: 缺少每个算子的输入输出日志和中间产物保存,无法复现问题,无法定位具体算子的失败原因。

**官方做法**: 未明确,但通常会保存中间产物用于调试。

**影响范围**: 所有调试和问题定位场景

**修复方案**: 在主流程中增加中间产物保存,包括:
- `content_list_converted.json` - 格式化后的输入
- `chunk_{i}_llm_output.xml` - 每个 chunk 的 LLM 原始输出
- `chunk_{i}_parsed.json` - 每个 chunk 的解析结果
- `merged_qa_pairs.json` - 合并后的问答对

**预计工作量**: 4 小时

---

#### 问题 7: 缺少质量评估指标

**位置**: 结果生成阶段

**问题描述**: 缺少质量评估指标,无法快速判断抽取质量是否正常。

**建议指标**:
- 每个 chapter 的题目数量分布
- label 连续性检查 (是否有跳号)
- question/answer/solution 的平均长度
- 图片引用的完整性

**修复方案**: 实现 `generateDiagnostics()` 和 `diagnosticsToMarkdown()` 函数,自动生成质量评估报告。

**预计工作量**: 4 小时

---

## 优先级行动计划

### 立即执行 (P0 - 数据丢失风险)

| 任务 | 预计工作量 | 负责人 | 截止日期 |
|------|-----------|--------|----------|
| 1. 移除输入阶段的目录过滤 | 2 小时 | 待分配 | D+1 |
| 2. 增加字段级别的增量更新 | 4 小时 | 待分配 | D+2 |
| 3. 实现二次提示机制 | 2 小时 | 待分配 | D+1 |

**总计**: 8 小时 (1 个工作日)

---

### 短期优化 (P1 - 稳定性风险)

| 任务 | 预计工作量 | 负责人 | 截止日期 |
|------|-----------|--------|----------|
| 4. 简化合并索引结构 | 4 小时 | 待分配 | D+3 |
| 5. 动态调整 max_tokens | 2 小时 | 待分配 | D+3 |

**总计**: 6 小时 (0.75 个工作日)

---

### 中期改进 (P2 - 可观测性)

| 任务 | 预计工作量 | 负责人 | 截止日期 |
|------|-----------|--------|----------|
| 6. 增加中间产物保存 | 4 小时 | 待分配 | D+7 |
| 7. 实现质量评估指标 | 4 小时 | 待分配 | D+7 |

**总计**: 8 小时 (1 个工作日)

---

## 验证方法

### 回归测试

使用当前测试任务 (八上数学测试.pdf) 验证改进效果:

1. **对比题目数量**: 改进前 225 道 → 改进后预期 230-250 道 (减少漏提取)
2. **对比章节分布**: 验证 20.1 和 20.2 章节的题目数是否增加
3. **对比 label 连续性**: 验证是否减少了 label 跳号
4. **对比内容完整性**: 验证 question/answer/solution 的平均长度是否增加

### 边界测试

使用包含以下特征的 PDF 进行测试:

1. **选项列表**: 验证选择题选项是否被正确提取
2. **目录页**: 验证目录条目是否被正确过滤
3. **复杂排版**: 验证双栏、跨页题目是否被正确提取
4. **Interleaved 模式**: 验证题目和答案在同一页的情况

### 质量指标

| 指标 | 改进前 | 改进后目标 | 验证方法 |
|------|--------|-----------|----------|
| 题目数量 | 225 | 230-250 | 统计 questions.json 的长度 |
| 空 chunk 比例 | 未知 | < 10% | 统计 diagnostics.json 的 emptyChunks |
| Label 跳号 | 未知 | < 5 处 | 统计 diagnostics.json 的 labelGaps |
| 平均题目长度 | 未知 | > 100 字符 | 统计 diagnostics.json 的 avgQuestionLength |
| 平均解答长度 | 未知 | > 200 字符 | 统计 diagnostics.json 的 avgSolutionLength |

---

## 风险评估

### 技术风险

| 风险 | 可能性 | 影响 | 缓解措施 |
|------|--------|------|----------|
| 改进后题目数量反而减少 | 低 | 高 | 先在测试环境验证,保留改进前的代码备份 |
| 二次提示导致 API 成本增加 | 中 | 中 | 只对空结果进行重试,不是所有 chunk 都重试 |
| 简化索引结构导致匹配失败 | 低 | 高 | 严格对齐官方逻辑,增加单元测试 |
| 动态 max_tokens 导致超出模型限制 | 低 | 中 | 设置上限 (如 32768),超出时拆分 chunk |

### 业务风险

| 风险 | 可能性 | 影响 | 缓解措施 |
|------|--------|------|----------|
| 改进周期过长影响项目进度 | 低 | 中 | 按优先级分阶段实施,P0 问题优先 |
| 改进后需要重新处理历史数据 | 高 | 低 | 提供数据迁移脚本,支持批量重新抽取 |
| 用户对改进效果不满意 | 低 | 中 | 提供诊断报告,量化改进效果 |

---

## 成功标准

### 短期目标 (1 周内)

1. ✅ 完成 P0 问题修复 (3 个问题)
2. ✅ 测试任务的题目数量增加 5-10%
3. ✅ 空 chunk 比例降低到 10% 以下
4. ✅ 提供完整的诊断报告

### 中期目标 (2 周内)

1. ✅ 完成 P1 问题优化 (2 个问题)
2. ✅ Label 跳号减少到 5 处以下
3. ✅ 平均题目长度和解答长度增加 10%
4. ✅ 通过边界测试 (选项列表、目录、复杂排版)

### 长期目标 (1 个月内)

1. ✅ 完成 P2 可观测性改进 (2 个问题)
2. ✅ 建立自动化测试流程
3. ✅ 提供完整的开发文档和 API 文档
4. ✅ 支持批量处理和增量更新

---

## 交付物清单

本次技术审查提供以下交付物:

1. **技术审查报告** (`analysis_report.md`) - 详细的算子级别对齐分析
2. **代码改进方案** (`code_improvements.md`) - 可直接落地的 TypeScript 代码
3. **执行摘要** (本文档) - 面向管理层的简明总结
4. **官方参考代码** - 已克隆到 `/home/ubuntu/DataFlow`
5. **目标项目代码** - 已克隆到 `/home/ubuntu/Mineru2Questions`

所有文档均使用 Markdown 格式,可直接在 GitHub 上查看或转换为 PDF。

---

## 联系方式

如有任何问题或需要进一步澄清,请联系:

**审查人员**: Manus AI Agent  
**审查日期**: 2026-02-08  
**项目地址**: https://github.com/shcming2023/Mineru2Questions  
**参考标准**: https://github.com/OpenDCAI/DataFlow  

---

**附注**: 本报告基于 2026-02-08 的代码快照进行审查,如项目代码有更新,部分结论可能需要重新评估。
