# Mineru2Questions 项目回归评审报告 (v2.0)

**评审结论：测试成功，去重功能已正确实现，核心数据链路稳定。**

本次修订 (`bf24c79`) 成功修复了 v1.9 报告中指出的两大缺陷，使系统产出稳定在 **846** 题，并实现了 **100% 准确**的跨 chunk 重叠区题目去重。这是一个重要的里程碑，标志着项目核心抽取、解析、去重链路已基本稳定。

在核心功能稳定的基础上，本次评审也清晰地暴露了当前提升覆盖率的主要瓶颈：**LLM 在处理长上下文时的注意力衰减问题**，导致部分 chunk 产出严重不足。这是后续优化的核心方向。

---

## 1. 核心指标与版本对比

| 指标 | v1.9 (去重不足) | v2.0 (去重正确) | 状态 |
| :--- | :--- | :--- | :--- |
| **最终产出** | 998 题 | **846 题** | ✅ **正常** |
| LLM 原始产出 | 902 题 | 902 题 | 稳定 |
| 去重/过滤丢失 | -152 题 (错误) | **56 题** | ✅ **准确** |
| 页面覆盖率 | 84.2% | 84.2% | 稳定 |

- **产出量符合预期**：从 LLM 原始输出的 902 题，精确去除了 56 道因 chunk 重叠导致的重复题目，最终得到 846 题，无任何质量过滤丢失。数据链路完整、可追溯。
- **去重完全修复**：v1.9 中“去重策略失效”和“过度去重”的问题已彻底解决。当前基于 `questionIds` 字符串的精确匹配策略 100% 准确地识别并去除了所有 56 个重复项。

## 2. 已修复缺陷验证 (Bugs Fixed)

| ID | 缺陷描述 | 验证结果 |
| :--- | :--- | :--- |
| **BUG-3** | `questionIds` 未导出到 JSON | ✅ **已修复**。`questions.json` 中已包含 `questionIds` 字段。 |
| **QTY-5** | 去重策略失效 | ✅ **已修复**。`deduplicateQuestions` 函数已正确实现基于 `questionIds` 的去重，效果符合预期。 |

## 3. 新发现的瓶颈：LLM 注意力衰减 (New Bottleneck)

本次评审定位到当前影响题目覆盖率的**首要瓶颈**：

- **P1 级性能瓶颈：LLM 注意力衰减导致内容遗漏**
  - **现象**：在处理包含大量题目的长 chunk 时，LLM 表现出严重的不稳定性。例如 `chunk_15` 和 `chunk_20`，两者均包含超过 40 道题目，但 LLM 都只输出了 **1** 道题，导致约 **85 道题目**被遗漏。
  - **根因**：这不是分块或 prompt 的问题，而是 LLM 模型自身在处理长上下文、多任务指令时的“注意力衰减”或“随机失败”现象。由于当前流程**没有重试（Retry）机制**，一旦 LLM 单次输出异常，这部分内容就永久丢失了。
  - **影响**：这是导致 17 个正文页面（`[8, 76-82, 88, 89, 93, 100, 104-108]`）未能覆盖的直接原因。

## 4. 与 DataFlow 官方流水线对齐分析

| 模块 | DataFlow 官方实现 | Mineru2Questions 当前实现 | 对齐差距与建议 |
| :--- | :--- | :--- | :--- |
| **重试机制** | `max_retries=5`，指数退避 | **无** | ❌ **严重缺失**。这是当前最需要对齐的功能。建议为 `callLLM` 函数增加基于 `axios-retry` 的重试逻辑，并加入**异常检测**（如 `qa_pair` 数量远低于预期时主动触发重试）。 |
| **去重策略** | `question_ids` 集合交集 | `questionIds` 字符串精确匹配 | ✅ **功能对齐**。M2Q 的策略更简单，但在此场景下已足够有效。 |
| **质量评估** | `qa_quality_filter` 算子 | `filterLowQuality` (当前无效) | ⚠️ **待对齐**。当前没有题目被质量过滤。建议后续根据需求，增加对“题干过短”、“答案缺失”等低质量产出的过滤规则。 |

## 5. 后续优化建议

1.  **【P1-立即修复】实现 LLM 调用重试机制**：
    - 在 `callLLM` 函数中集成 `axios-retry` 或类似库，设置 `retries: 3` 和指数退避策略。
    - 在 `processChunk` 中增加一个**健全性检查 (Sanity Check)**：如果一个 chunk 包含大量 blocks 但 LLM 输出的 `qa_pair` 数量为 0 或 1，应主动抛出错误以触发重试。这能有效挽回因 LLM 注意力衰减而丢失的 85 道题。

2.  **【P2-择机优化】改进章节标题提取逻辑**：
    - **问题**：当前章节标题质量不高，仍有噪声（如将题干误判为标题）。
    - **建议**：在 `prompts.ts` 中优化对 `MAIN_TITLE_ID` 的指导，要求 LLM 优先选择具有明确章节编号（如 `20.1`）、字体更大、或类型为 `title` 的 block 作为章节标题，减少其“自由发挥”的空间。

## 6. 总结

项目已进入一个**稳定的新阶段**。所有已知的 P0 级阻塞性缺陷均已修复，核心数据链路清晰、可靠。当前的主要矛盾已从“跑不通”转变为“跑不全”。

强烈建议**优先解决 LLM 重试机制的缺失**，这将是提升题目覆盖率最直接、最有效的手段。
